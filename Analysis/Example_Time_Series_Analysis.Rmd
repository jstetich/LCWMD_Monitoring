---
title: "Exploring Models for Time Series Analysis of LCWMD Chloride Data"
author: "Curtis C. Bohlen, Casco Bay Estuary Partnership."
date: "11/12/2020"
output:
  github_document:
    toc: true
    fig_width: 5
    fig_height: 3
---
<img
    src="https://www.cascobayestuary.org/wp-content/uploads/2014/04/logo_sm.jpg"
    style="position:absolute;top:10px;right:50px;" />

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center',
                      fig.width = 5, fig.height = 4,
                      collapse = TRUE, comment = "#>")
```

# Introduction
This R Notebook explores various models for analyzing chloride levels in Long
Creek. Our principal goal is NOT to predict future values, but to assess the
contribution of certain (time varying) predictors to explaining the pattern in
the time series.

This notebook looks principally at chlorides, and a single site, to explore
model structure.  We explored dozens of prior models before settling on the
structure we are using here. We are especially interested in looking at whether
there have been long-term trends in water quality over the duration of the LCWMD
monitoring program.

The key insight is that we can use generalized least squares or generalized
additive models with an auto regressive error term. This motivates most of the
models explored here.

We explore time series methods to assess temporal trends in the LCWMD data.
Simple linear models of the LCWMD data are based on the assumption that
observations are independent, however, we know both on principal and from
working with the data, that the data are both auto-correlated and
cross-correlated in complex ways.

One challenge to analyzing the original data from LCWMD was INCOMPLETE, in the
sense that it was missing data from certain days or times. The simplest time
series methods in base R assume complete data in terms of how the data is laid
out -- even if many values are NA.  The time series is assumed to be a sequence
of observations equally spaced.

One solution is to use the `zoo` package, which extends time series methods in
base R to indexed series, where the index can be any value that supports
ordering. Or you can use the `xts` package, which extends `zoo`.  But we found
those tools of marginal value in the context of developing complex linear and
GAM models with autoregressive error terms.  The principal limit we ran into was
the inability of the modeling tools to run large models with autoregressive
errors, in reasonable time, or in some cases, at all.

We ended up constructing complete (or nearly complete) time series as an
alternative to `zoo` and `xts`. After exploring base TS methods and zoo methods,
we fell back on linear and GAM models, with lags and weighted sums of recent
rainfall calculated in advance.

# Import Libraries  
```{r}
library(tidyverse)
library(readr)
library(emmeans)   # Provides tools for calculating marginal means

library(nlme)      # includes the gls function, which simplifies weighted LS

library(zoo)       # provides utilities for working with indexed time series,
                   # critically here, the `rollapply()` function

library(mgcv)      # One of two common libraries for generalized additive models.
                   # Function gamm allows autocorrelation.

library(CBEPgraphics)
load_cbep_fonts()
theme_set(theme_cbep())

```

# Functions for Weighted SUms
Here we create a couple of functions to calculate weighted sums of recent
precipitation.  We only use `expweights()`, below, but used both in early model
explorations. Further details are provided in the data import code in the
`Derived_Data` folder.

*  `linweights()` Calculates a weighted sum from a time series.  The weighted
   sum is based on a window of ten observations. The weighted sum weights the 
   current (last) value zero, and adds a weighted sum where the earliest
   sample is weighted `rate` (O.1 by default), the second is weighted 
   `2 * rate`, etc., up to the ninth observation, which is weighted `9 * rate`.

*  `expweights()` weights each observation by a multiplicative factor, so
   each sample i from 1 to 9 is weighted `rate ^ (10 - i)`, so the ninth sample
   is weighted rate, the eighth is rated `rate ^ 2`, and so on, to the first
   sample, which is weighted `rate ^ 9`.  For rate a fraction, this strongly
   downweights older samples.
```{r}
linweights <- function(x, rate=.1) {
  stopifnot(length(x)==10)
  out = 0
  for (i in seq_len(length(x)-1)) {out<-out+x[i]*(rate)*(i)}
  return(out)
}


expweights <- function(x, rate=(4/5)) {
  stopifnot(length(x)==10)
  out = 0
  for (i in seq_len(length(x)-1)) {out<-out+x[i]*(rate)^(10-i)}
  return(out)
}

# Error checking code, to make sure these are working appropriately.
check <- c(1,0,0,0,0,0,0,0,0,1)
rollapply(check, 10, linweights)
rollapply(check, 10, expweights)
(4/5)^9

check=c(0,0,0,0,0,0,0,0,0, 1,0,0,0,0,0,0,0,0,0)
rollapply(check, 10, linweights)
rollapply(check, 10, expweights)
```
# Import Data
## Folder References
```{r}
sibfldnm    <- 'Derived_Data'
parent      <- dirname(getwd())
sibling     <- file.path(parent,sibfldnm)

dir.create(file.path(getwd(), 'figures'), showWarnings = FALSE)
```

## Data on Sites and Impervious Cover
These data were derived from Table 2 from a GZA report to the Long Creek
Watershed Management District, titled "Re: Long Creek Watershed Data Analysis;
Task 2: Preparation of Explanatory and Other Variables."  The Memo is dated
November 13, 2019 File No. 09.0025977.02.

```{r}
# Read in data and drop the East Branch, where we have no data
fn <- "Site_IC_Data.csv"
fpath <- file.path(sibling, fn)

Site_IC_Data <- read_csv(fpath) %>%
  filter(Site != "--") 

# Now, create a factor that preserves the order of rows (roughly upstream to downstream). 
Site_IC_Data <- Site_IC_Data %>%
  mutate(Site = factor(Site, levels = Site_IC_Data$Site))

# Finally, convert percent covers to numeric values
Site_IC_Data <- Site_IC_Data %>%
  mutate(CumPctIC = as.numeric(substr(CumPctIC, 1, nchar(CumPctIC)-1))) %>%
  mutate(PctIC = as.numeric(substr(PctIC, 1, nchar(PctIC)-1)))
Site_IC_Data
```

## Main Data
Read in the data from the Derived Data folder.

Note that I filter out data from 2019 because that is only a partial year, which might affect estimation of things like seasonal trends.  We could add it back in, but with care....
```{r}

fn <- "Full_Data.csv"
fpath <- file.path(sibling, fn)

full_data <- read_csv(fpath, 
    col_types = cols(DOY = col_integer(), 
        D_Median = col_double(), Precip = col_integer(), 
        X1 = col_skip(), Year = col_integer(), 
        lD_Median = col_double())) %>%

  mutate(Site = factor(Site, levels=levels(Site_IC_Data$Site))) %>%
  mutate(Month = factor(Month, levels = month.abb)) %>%
  mutate(IC=as.numeric(Site_IC_Data$CumPctIC[match(Site, Site_IC_Data$Site)])) %>%
  mutate(Yearf = factor(Year))
rm(Site_IC_Data)
rm(fn, fpath, parent, sibling, sibfldnm)
```

# Simple Plot of Chloride Levels
```{r}
ggplot(full_data, aes(sdate, Chl_Median)) + geom_point(aes(color=Site), alpha=0.25) +
  geom_smooth(aes(group = Yearf), se=FALSE,
              method = 'gam',
              formula = y~s(x, k=3)) +  # span is essentially arbitrary....
  ylab('Chloride (mg/l)') +
  xlab('Date') +
  ggtitle('Daily Medians') +
  theme_minimal()
```

Note that the smooth fits here really don't make much sense, since the data
across multiple sites are combined, but it's useful for assessing whether the
data is in the format we want.  Still note the general downward trend in most
years.

# Data for Site 17
```{r}
the_data <- full_data %>% filter(Site=='S17')
```

# Explore Autocorrelation Structure
Now that we have a zoo, we can look at autocorrelation and crosscorrelation.
But since we pull data out of the zoo objects using coredata(), this is the same
as running the analysis on a non-zoo object.  So these analysis don't handle 
seasonal patterns or winter data gaps correctly. These are preliminary analyses
only.
```{r}
oldpar <- par(mfrow=c(2,3), mar = c(1,2,3,1))
acf(the_data$Precip, main = 'Precip', na.action= na.pass)
acf(the_data$D_Median, main='Depth', lag.max=100, na.action= na.pass)
acf(the_data$Chl_Median, main='Est. Chloride (mg/l)', lag.max=100, na.action=na.pass)
pacf(the_data$Precip, main = 'Precip', na.action= na.pass)
pacf(the_data$D_Median, main='Depth', lag.max=100, na.action= na.pass)
pacf(the_data$Chl_Median, main='Est. Chloride (mg/l)', lag.max=100, na.action=na.pass)
par(oldpar)
```
So, this suggests first order autoregressive process for the Chloride and depth
metrics. Precipitation would probably be better fit with a first order moving
average process.  Prior work has shown similar patterns looking at
log-transformed values.

# Explore Cross Correlation Structure
```{r}
oldpar <- par(mfrow=c(1,2), mar = c(1,2,3,1))
a <- ccf(the_data$Chl_Median, the_data$Precip, 
    main = 'Chloride by Precipitation',
    lag.max = 10,
    ylim = c(-.6,0.1), 
    na.action= na.pass)
b <- ccf(the_data$Chl_Median, the_data$D_Median, 
    main = 'Chloride by Depth',
    lag.max = 10,
    ylim = c(-.6, 0.1),
    na.action= na.pass)
par(oldpar)
```
```{r}
a[-2:10]
b[-2:10]
```

Chlorides are negatively correlated with rainfall.  The connection is only
moderate with rainfall on the day of the observation, but stronger with rainfall
from several days before.  We see significant correlations going back about 10
days.

Chlorides are strongly (negatively) correlated with depth, with a  peak at 
a lag of zero. Correlations are nearly symmetrical for leading and lagging 
depths.

# Log Linear Models
These models are technically incorrect, as they have autocorrelated errors, but
they allow us to select the right autocorrelation structure as we move to more
sophisticated models.  While we're at it, we explore whether to treat 
seasonality by day of year or by month.

(Prior work showed that analysis of log transformed data better conforms to 
model assumptions. We do not show similar models of untransformed data here.)

## Weighted and Transformed Precipitation data
Note that `lprecip`, `wprecip`, and `wlprecip` were part of the the 
data we imported. Code creating them can be found in the 
`Make_Complete_Data.Rmd` notebook.  They correspond to:  
*  `lprecip`: Log of Precip Data.  
*  `wprecip`: Weighted prior precipitation, weighted according to the 
   `expweights()` function.  
*  `wlprecip` Weighted logs of prior precipitation, weighted according to the 
   `expweights()` function.  
   
Similarly, `lD_Median` is a term for the log of the daily median water depth.
   
## Polynomial by Day of Year
Data from `Site == S17` has only been available since 2015, so we are fitting
only four years of data here (2015 through 2018).
```{r}
Chl_llm_1 <- lm(log(Chl_Median) ~ Yearf + lPrecip +
                  wlPrecip + lD_Median +
                  poly(DOY,5), data = the_data,
                  na.action=na.omit)
summary(Chl_llm_1)
```
Chloride levels vary year to year, even after addressing precipitation and
depth.  The time of year clearly does matter.

## Strictly Periodic (sine and cosine) fit.
```{r}
Chl_llm_2 <- lm(log(Chl_Median) ~ Yearf + lPrecip + wlPrecip + lD_Median +
                  sin(2*pi*DOY/365) + cos(2*pi*DOY/365) +
                  sin(2*pi*DOY/(2*365)) + cos(2*pi*DOY/(2*365)),
                  data = the_data, na.action=na.omit)
summary(Chl_llm_2)
```
Fitting a seasonal pattern by sine and cosine functions again shows time of year
matters.  A periodic fit changes other coefficients (for year, precipitation, 
and depth)  only slightly, suggesting the exact form of the seasonal fit may 
not much alter the types of long-term trends we observe.

## Genealized Addative Model Fit
```{r}
Chl_llm_3 <- gam(log(Chl_Median) ~ Yearf + lPrecip +
                  wlPrecip + lD_Median +
                  s(DOY, k=6), data = the_data,   # k here is arbitrary, to get 
                                                  # a fairly simple curve
                  na.action=na.omit)
summary(Chl_llm_3)
```

And using a generalized linear model again has little effect on our conclusions 
about the non-seasonal model parameters.

## Seasons by Month, not Day of Year
```{r}
Chl_llm_4 <- lm(log(Chl_Median) ~ Yearf + lPrecip +
                  wlPrecip + lD_Median +
                  Month, data = the_data,
                  na.action=na.omit)
summary(Chl_llm_4)
```

This slightly lowers our estimates of precipitation and depth parameters.
```{r}
anova(Chl_llm_2, Chl_llm_3, Chl_llm_1, Chl_llm_4)
```

```{r}
AIC( Chl_llm_2, Chl_llm_3, Chl_llm_1,  Chl_llm_4)
```
The sine and cosine model is the best both by residual sums of squares, and by 
AIC, but the differences are small. The GAM model can be improved by allowing 
higher degrees of freedom for the seasonal curve.  Use of GAMM also allows
easier model development.

The GAM fit needs to be tuned to avoid generating a very wiggly line.  Here,
we've fit a GAM that has about the same degrees of freedom as the polynomial
fit. What did that smooth look like?  (Note that the degree of wiggliness will
change as we move towards better models).
```{r}
plot(Chl_llm_3)
```

## Autocorrelation of residuals
This is a partially incorrect analysis, because ACF and PACF don't handle 
missing values correctly. Still, given the richness of our data, it is adequate 
for our current question.
```{r}
oldpar <- par(mfrow=c(1,2),mar = c(1,2,3,2))

r <- resid(Chl_llm_3)

acf(r, na.action=na.omit, main= 'Model Residuals')
pacf(r, na.action=na.omit, main= 'Model Residuals')
par<-oldpar
```

```{r}
rm(Chl_llm_1, Chl_llm_2, Chl_llm_3, Chl_llm_4)
```

## Examining Autocorrelation of Residuals
```{r}
a<-acf(r, na.action=na.omit, main= 'Model Residuals', plot=FALSE)
(theautocor <- a$acf[2])
```
So we can use an auto regressive model of degree one, with a correlation near 
0.8.

# Generalized Least Squares
The simplest way to handle autocorrelated errors is with generalized least 
squares.  This is a simple version of this model, without interactions,
and using a fifth degree polynomial to capture the seasonal effect.
```{r}
the_gls <- gls(log(Chl_Median) ~ Yearf + lPrecip + wlPrecip + lD_Median +
                 poly(DOY,3), data = the_data, na.action=na.omit,
                 correlation=corAR1(0.8))
summary(the_gls)
```
The correlations among parameters suggest the interdependence of the Year
factors, which is expected. Negative correlations among depth and the
precipitation variables also makes sense.  We expect depth to be correlated with
precipitation variables, so the negative correlation among the parameters is
expected. If one parameter goes up, the ideal fit for the other should go down.

Note that this model fits a SEQUENTIAL autocorrelation function, not an indexed
autocorrelation function.  That is, this is not a fully time-aware model. We
still need to account for annual breaks in coverage, or otherwise handle that.

```{r} 
anova(the_gls)
```
Notice that the effect of TODAY's rainfall is not statistically significant when
judged by T test, but IS significant by ANOVA.

## Diagnostic Plots
```{r}
plot(the_gls)
qqnorm(the_gls, abline=c(0,1))
plot(the_gls, Yearf~resid(., type='p'))
plot(the_gls, resid(., type='p')~DOY)
plot(the_gls, resid(., type='p')~lPrecip)
```
Extreme residuals are a problem.  They mostly correspond to low values observed
in 2018 over a period of a few days. They may affect our models, but probably
not excessively.  We should consider measures of statistical significance
with care.

# Generalized Linear Models
The gamm() function in the mcgv package handles GAMs with an `ar1()` autocorrelation
quite handily.  There are a large number of potential model structures that we
might want to use here, so I explore many of them here.

We use `method = 'ML'' while exploring models, because the fixed components of 
the models change under 'REML', so comparing models produced with
`method = 'REML'` would generate nonsense.  After selecting a model, we refit 
the model with REML to improve performance.

We start by fitting simple models with only a single GAM term for the Day of the 
Year.  This gives us a way to explore other components of model structure.

Note that as we move towards a GAM fit with an AR1 term, the optimal smoothed 
fit for Day of Year changes.

## Basic Model Sequence
These are a basic series of (nested) interaction models, which we can compare by
ANOVA or AIC.  Here we are trying to select the preferred interaction structure
among our precipitation related predictor variables.

There is some ambiguity here on whether to fit cyclic cubic splines with
`bs = 'cc'` or not.  We chose not to, as we do not have full 365 day annual
records.  In other words, there is a gap in time between the end of one year's
data and the beginning of the following year's data.  Forcing a cyclic smoother
under those circumstances makes a possibly unwarranted assumption about 
continuity across the winter break in monitoring.
```{r}
# No Interactions
gam_fit_1 <- gamm(log(Chl_Median) ~ Yearf + 
                    lPrecip + 
                    wlPrecip + 
                    lD_Median +
                s(DOY),
                data = the_data, na.action=na.omit, method='ML',
                correlation=corAR1(0.8))

# Weighted precipitation by depth interaction only
gam_fit_2 <- gamm(log(Chl_Median) ~ Yearf + 
                  lPrecip + 
                  wlPrecip * lD_Median +
                s(DOY),
                data = the_data, na.action=na.omit, method='ML',
                correlation=corAR1(0.8))

# Both precipitation by depth interactions
gam_fit_3 <- gamm(log(Chl_Median) ~ Yearf + 
                    lPrecip + 
                    wlPrecip + 
                    lD_Median +
                    lPrecip:lD_Median +
                    wlPrecip:lD_Median +
                    s(DOY),
                  data = the_data, na.action=na.omit, method='ML',
                  correlation=corAR1(0.8))

# Both precipitation by depth interactions, as well as current precipitation
# by weighted past precipitation interactions.
gam_fit_4 <- gamm(log(Chl_Median) ~ Yearf + 
                    (lPrecip + wlPrecip + lD_Median)^2 +
                    s(DOY),
                  data = the_data, na.action=na.omit, method='ML',
                  correlation=corAR1(0.8))

# Add the three way interaction.
gam_fit_5 <- gamm(log(Chl_Median) ~ Yearf + 
                    lPrecip * wlPrecip * lD_Median +
                    s(DOY),
                  data = the_data, na.action=na.omit, method='ML',
                  correlation=corAR1(0.8))
```

Comparing these models, it is clear that interaction terms help with model
fitting. That is probably not unreasonable, since today's rainfall could well
interact with yesterday's rainfall or the current water depth in complex ways.

```{r}
anova(gam_fit_1$lme, gam_fit_2$lme, gam_fit_3$lme, gam_fit_4$lme, gam_fit_5$lme)

```
Models 3,4, and 5 provide very similar (not statistically significant)
differences in performance by AIC or Log likelihood.  Model 4 is SLIGHTLY better
by AIC, model 3 by BIC.

From a practical point of view, given our goals (to facilitate detection of
differences among years), any of the last three models are functionally similar.
We like model 3, because of its parsimony.

Note that having done a better job of  the flow characteristics, the GAM 
term comes back as a linear function of time of year.  That is because the
`mgcv` package implements thin-plate splines, by default, which "shrink" the
spline fit, and reduce degrees of freedom. That means weare functionally fitting
a simple linear model here.

```{r}
plot(gam_fit_3$gam, main="Thin  Plate Spline")
```

```{r}
gls_fit_3 <- gls(log(Chl_Median) ~ Yearf + DOY +
                    lPrecip + 
                    wlPrecip + 
                    lD_Median +
                    lPrecip:lD_Median +
                    wlPrecip:lD_Median,
                  data = the_data, na.action=na.omit, method='ML',
                  correlation=corAR1(0.8))
```

```{r}
AIC(gls_fit_3)
anova(gls_fit_3)
```


```{r}
anova(gam_fit_3$gam)
```

There is some evidence or problems with colinearity here, where hydraulic 
variables and time of year are intercorrelated.


```{r}
p1 <- predict(gls_fit_3)
formula(gls_fit_3)
p2 <- predict(gam_fit_1$gam)
formula(gam_fit_1$gam)
p3 <- predict(gam_fit_3$gam)
formula(gam_fit_3$gam)
p4 <- predict(gam_fit_4$gam)
formula(gam_fit_4$gam)
p5 <- predict(gam_fit_5$gam)
formula(gam_fit_5$gam)
```

```{r}
cor(cbind(p1, p2,p3,p4,p5))
```
So, all models are providing predictions with better than 99.5% correlations.

The GLS and Model 3 are equivalent.

For most purposes, we should probably select for parsimony.  Note
especially that the GLS fit is correlated at over 99% with each of the GAM 
models.


```{r}
rm(gam_fit_1, gam_fit_2, gam_fit_3, gam_fit_4, gam_fit_5)
rm(p1, p2, p3, p4, p5)
```


## Refining the Model
We continue fitting models based on Model 3, testing additional possible GAM
components in place of linear and interaction terms.

IN normal linear models, one does not include an interaction term without also 
including both main effects to which it contributes (with some exceptions).
The equivalent constraint here is to not include an "interaction" tensor smooth 
(with function `ti()`) without including the associated "main effects" tensor
components too. If one only wants to construct the combined smooth without 
testing the structure of the interaction, you can do so with tyhe function 
`te()`.

But then we face a problem if we want linear interactions with one predictor
and smoothed interaction with another.  On some level, that makes mathematical
sense, but may not be interpretable.  Her we stick to either fitting smoothed or
linear terms related to each predictor.
```{r}
# Linear Interactions (same as prior model 3)
gam_fit_3.1 <- gamm(log(Chl_Median) ~ Yearf + 
                      lPrecip + 
                      wlPrecip + 
                      lD_Median +
                      lPrecip:lD_Median +
                      wlPrecip:lD_Median +
                      s(DOY),
                data = the_data, na.action=na.omit, method='ML',
                correlation=corAR1(0.8))

# Smooth current depth; linear interaction past precip and depth
gam_fit_3.2 <- gamm(log(Chl_Median) ~ Yearf +
                      lPrecip + 
                      wlPrecip +
                      wlPrecip:lD_Median +
                      s(lD_Median)+ 
                      s(DOY),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))

# Smooth current depth; linear interaction current precip and depth
gam_fit_3.3 <- gamm(log(Chl_Median) ~ Yearf + 
                      lPrecip + 
                      wlPrecip + 
                      lPrecip:lD_Median +
                      s(lD_Median)+ 
                      s(DOY),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))

# Smooth depth, Precipitation and their interaction.
# and depth
gam_fit_3.4 <- gamm(log(Chl_Median) ~ Yearf + 
                        wlPrecip + 
                        ti(lPrecip) +
                        ti(lD_Median) + 
                        ti(lPrecip, lD_Median) +
                        s(DOY),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))

# Smooth depth, past Precipitation and their interaction
gam_fit_3.5 <- gamm(log(Chl_Median) ~ Yearf + 
                        lPrecip + 
                        ti(wlPrecip) +
                        ti(lD_Median)+ 
                        ti(wlPrecip, lD_Median) +
                        s(DOY),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))

# Interaction smooths
gam_fit_3.6 <- gamm(log(Chl_Median) ~ Yearf +
                      ti(lPrecip) +
                      ti(wlPrecip) +
                      ti(lD_Median) +
                      ti(lPrecip,  lD_Median)+
                      ti(wlPrecip, lD_Median) +
                      s(DOY),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))
```


```{r}
anova(gam_fit_3.1$lme, gam_fit_3.2$lme, gam_fit_3.3$lme, 
      gam_fit_3.4$lme,  gam_fit_3.5$lme, gam_fit_3.6$lme)
```
So, model 3.4 and 3.6 are the best, and they are also better than the 
(linear) GLS fit, which has AIC = -636.8369.

## A Closer Look at the Full Model
```{r}
summary(gam_fit_3.6$gam)
```

According to that, all three smooth terms and both interaction terms are 
significant, and should be retained in the model.  But this leads to a complex
model that will be slow to fit when we look at all sites.

```{r fig.width = 7, fig.height = 5}
plot(gam_fit_3.6$gam)
```
So current precipitation matters, but only for  high values of current
precipitation. lPrecip = log(1 + x) for x = Precip, so values at log(6),
where rainfall appears to matter, correspond to about `r exp(6)` tenths of 
millimeters of rainfall, or about 4 cm, close to one and one half inches. 

Historic rainfall matters, across all levels, but is adequately modeled by a 
linear function.

Stream depth is an important predictor, except at high depths.

The interaction terms are hard to address.  Depth by precipitation matters
for high rainfall on days of deep water, or conversely, low rainfall on days of 
high water. Dilution?  Snow melt?

There is less and less of an effect of high historic rainfall as stream depth
increases.

### Model Checks
```{r}
gam.check(gam_fit_3.6$gam)
```
Both historic water level and day of year can be adequately modeled as linear
functions.  But if we go that route, we can not fit full GAM interactions 
using tensor products.

Looking at residuals, we again have very heavy tails.  That is problematic.  We
are lucky that the high residual terms do not appear to have high leverage.
Note that in this model, once we address precipitation and depth, time of year 
is also adequately modeled with a linear function.

# Revised Models
Taking those lessons, we drop the smoothing term for Day of Year and historic
precipitation, and combine the other smoothed terms.  This produces models that 
are similar to 3.4, which was our second-best performing model.

We also test  use of different ways of defining smoothing terms using classic
smooths (`s()`), tensor product smooths (`te()`) and independent tensor product
smooths (`ti()`) to specify (conceptually) similar models. They turn out to not
be as similar as we thought.

This first model simply takes 3.4 and shifts the DOY term to  linear predictor.
Since 3.4 settled on a linear predictor for the smoothed DOY term, the 
predictions, and thus log likelihood are identical, but we save a degree of
freedom
```{r}
gam_fit_3.7 <- gamm(log(Chl_Median) ~ Yearf +
                      wlPrecip +
                      DOY +
                      ti(lPrecip) +
                      ti(lD_Median) + 
                      ti(lPrecip, lD_Median),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))
```

Note that all three tensor smooth terms are important.
```{r}
anova(gam_fit_3.7$gam)
```
```{r}
plot(gam_fit_3.7$gam)
```

Depth could ALMOST be replaced by a linear function here.

This next model takes the prior model (3.7), and replaces the three independent
tensor smooths with a single joint tensor smoother.  However, predictions are
not identical, producing a slightly lower log likelihood. Online comments from
stats.stackexchange:

https://stats.stackexchange.com/questions/234809/r-mgcv-why-do-te-and-ti-tensor-products-produce-different-surfaces

suggest the reason is that the underlying fitting strategy differs, with more
penalty matrices used to estimate more smoothing terms for the tensor 
interaction models.  Comments in that same thread suggest it is reasonable to
go with a combined tensor model when examination of the tensor interaction model
suggests a joint smooth is needed,as here.
```{r}
gam_fit_3.8 <- gamm(log(Chl_Median) ~ Yearf +
                      wlPrecip +
                      DOY +
                      te(lPrecip,  lD_Median),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))
```

Finally, we generate a similar joint smoother that is NOT a tensor smooth. This
effectively scales the smoother identically in both dimensions, which is
suboptimal where depth and precipitation are in different (although log) units.
```{r}
gam_fit_3.9 <- gamm(log(Chl_Median) ~ Yearf +
                      wlPrecip +
                      DOY +
                      s(lPrecip,  lD_Median),
                 data = the_data, na.action=na.omit, method='ML',
                 correlation=corAR1(0.8))
```


```{r}
anova(gam_fit_3.4$lme, gam_fit_3.7$lme,  gam_fit_3.8$lme,  gam_fit_3.9$lme)
```
Model 3.7 is the "best" model by AIC, and 3.8 is the "best" by BIC. The only
difference is in how we fit the smoothed term.  In our intended use (looking at 
multiple sites), we won't check the importance of sub-terms, so it makes sense
to use a model of the form of 3.8. That has the added advantage of making it 
easier to understand the form of the bivariate smoothing function.

```{r}
plot(gam_fit_3.8$gam)
```

Again, that emphasizes that effect of strra mdepth is nearly linear.

## Compare Predictions
```{r}
p4 <- predict(gam_fit_3.4$gam)
formula(gam_fit_3.4$gam)
p5 <- predict(gam_fit_3.5$gam)
formula(gam_fit_3.5$gam)
p6 <- predict(gam_fit_3.6$gam)
formula(gam_fit_3.6$gam)
p8 <- predict(gam_fit_3.8$gam)
formula(gam_fit_3.8$gam)
```

```{r}
cor(cbind(p4, p5, p6, p8))
```
All of our predictions are highly correlated with each other, so it can make 
little difference which model we choose.

```{r fig.width = 7}
plot(p8~p6, main='GAM 3.8 te(Precip x Depth) Compared to Full Model')
abline(0,1)
```

# Conclusion:
Best general model is 3.8.  3.6 is a slightly more general model.  A fully
generalized model (three way tensor interaction) would also be valuable,
but would be likely to be slow to fit to our complete data.
```{r}
formula(gam_fit_3.4$gam)
formula(gam_fit_3.8$gam)
formula(gam_fit_3.6$gam)
```

# Refit model using REML
```{r}
gam_fit <- gamm(log(Chl_Median) ~ Yearf +
                      wlPrecip +
                      DOY +
                      te(lPrecip,  lD_Median),
                 data = the_data, na.action=na.omit, method='REML',
                 correlation=corAR1(0.8))
```

# And a similar GLS model
We add a quadratic term for the lD_Median to allow for curvature of the
response, like what we saw in the GAM.
```{r}
gls_fit <- gls(log(Chl_Median) ~ Yearf +  DOY +
               lPrecip + wlPrecip + lD_Median + 
               lPrecip:lD_Median,
               data = the_data, na.action=na.omit,
               correlation=corAR1(0.8))
```

```{r}
summary(gls_fit)
```


```{r}
p_gam <- predict(gam_fit$gam)
p_gls <- predict(gls_fit)

cor(cbind(p_gam, p_gls))
```
So, that GLS fit is slightly different, especially at lower predicted chloride
levels.  We probably should stick to the GAM fit, despite its greater conplexity.

```{r fig.width = 7}
plot(p_gls ~ p_gam, main='GLS vs GAM Model Fits')
abline(0,1)
```


# Extracting Adjusted Means
We use the emmeans package to adjust predictions for other covariates.  However, the package is not behaving similarly for bothe the GAMM and GLS fits.

## Simplest call for the GAMM fit
Here we need to provide information to allow emmeans to extract values. The 
`quote()` function quotes it's argument -- that is, passes it on unevaluated.
```{r}
(f <- gam_fit$gam$formula)
```

```{r}
t1 <- emmeans(gam_fit, ~Yearf, data = the_data)
t1
```
We really want all of this back-transformed to raw concentrations, not in log
scale. For some reason, emmeans is not providing the back transform
when the parameter `type = 'response'` is set. (So we removed the parameter 
while we work this out.)

Working through the emmeans 'transformations' vignette points the way.  We need 
to explicitly tell `emmeans()` about the transformation. We do that by
creating a reference grid and updating it directly.

```{r}
my_rg <- update(ref_grid(gam_fit, data = the_data),
                tran = 'log')
my_rg
```

```{r}
tt1 <- emmeans(my_rg, "Yearf", type = 'response')
summary(tt1)
```

But notice that the grid locations are perhaps not what we want. In particular,
this has estimated marginal means for a day with the following properties:

*  Precipitation of about `r round(exp(1.195) - 1,2)` mm,  
*  (Weighted) recent precipitation of a whopping `r round(exp(4.099) - 9,2)`,  
*  estimates stream depth of `r round(exp(0.201) - 1,2)` meters.  
*  all for the 203rd day of the year.



These are selected based on MEAN values, and for the types of skewed predictors
we have, we are better off working with medians.  While we are at it, we
generate marginal means for the 200th day of the year, which is either July 1 or
2, depending on whrether it's a leap year or not.

We can check the Julian Day of the first of July (2020, a leap year) as follows:
```{r}
format(as.Date('01-07-2020'), '%j')
```

```{r}
my_rg_2 <- update(ref_grid(gam_fit, data = the_data,
                           cov.reduce=median,
                           at = list(DOY = 200)),
                tran = 'log')
my_rg_2
```

```{r}
tt2 <- emmeans(my_rg_2, "Yearf", type = 'response')
summary(tt2)
```

Thus under "median" conditions, we expect slightly higher
chloride than under mean conditions.


We can even specify the locations for estimating marginal means entirely
manually.

```{r}
my_rg_3 <- update(ref_grid(gam_fit, 
                         at = list(Yearf =c(2015, 2016, 2017, 2018),
                                   wlPrecip = log(10),   # half a cm of weighted rainfall
                                   DOY = 200,  # June 30th in leap years.
                                   lPrecip = 0,   # NO rain today.
                                   lD_Median = 0.2),
                         data = the_data),
                tran = 'log')
my_rg_3
```


```{r}
tt3 <- emmeans(my_rg_3, "Yearf", type = 'response')
summary(tt3)
```


## Some Graphics Alternatives

```{r}
plot(tt2, horizontal = FALSE, comparisons=TRUE) + 
  xlab('Year')+ ylab('Chlorides (mg/l)') + ggtitle('Adjusted Geometric Means') +
  theme_minimal()
```

```{r}
pwpm(tt2)
pwpp(tt2)
```

## An Alternative Model
We generate emmeans from the GLS model as well, just to demonstrate that for our 
purposes, the results are similar. 
```{r}
 emmeans(gls_fit, ~Yearf, mode = "df.error", 
              cov.reduce=median, at = list(DOY = 200),
              type = 'response')
```
Note that the degrees of freedom are calculated differently, but the standard 
errors and confidence intervals are not very different.

# Decomposing Model Results
We want to decompose the results into:
0. The Raw data
1. A YEAR BY YEAR component
2. A DAY OF YEAR component
2. A WEATHER component, including rainfall and flow
4. An ERROR Term.

The challenge here is that we want to use the ggplot facet system, which
requires a tidy dataset

The way to do this is to fit sequentially simpler models, on the
residuals of prior models.

We have just completed the Year analysis, so we rely on that.

## Raw Data
We center the data before plotting....
```{r}
plotdat <- the_data %>%
  select(sdate, Chl_Median, lPrecip, wlPrecip, lD_Median, Yearf, DOY) %>%
  filter(complete.cases(.))  %>%    # We don't need the missing values anymore.
  mutate(cObserved = scale(Chl_Median, center=TRUE, scale=FALSE))
```

## Year by Year
```{r}
(q <- summary(tt2))
```

```{r}
plotdat <- plotdat %>%
  mutate(YrAdjust = q[match(Yearf, q$Yearf), 2],
         aChl_Median = Chl_Median - YrAdjust,
         cYrAdjust = scale(YrAdjust, center=TRUE, scale=FALSE))
```

## Day of Year....
The way we do that is to fit another model, to the annually adjusted chloride
data, extract the emmeans, and continue. Day of year term should be linear, 
since that is what we fitted, above.

But I'm having trouble fitting these data to the model we used before, 
because of convergence issues.

gamm(log(Chl_Median) ~ Yearf +
                      wlPrecip +
                      DOY +
                      te(lPrecip,  lD_Median),



```{r}
doy_fit <- gamm(log(aChl_Median) ~ 
                    wlPrecip +
                    DOY +
                    te(lPrecip, lD_Median),
                data = plotdat, 
               na.action=na.omit, 
               method = 'REML',
                correlation=corAR1())
```

```{r}
my_rg_5 <-  update(ref_grid(doy_fit, data = the_data,
                            cov.keep = 'DOY',
                            cov.reduce=median),
                tran = 'log')
```


```{r}
t5 <- emmeans(my_rg_5, ~ DOY, type='response')
```

```{r}
q <- summary(t5)

plotdat <- plotdat %>%
  mutate(DOYAdjust = q[match(DOY, q$DOY),2],
         aChl_Median = aChl_Median - DOYAdjust,
         cDOYAdjust = scale(DOYAdjust, center=TRUE, scale=FALSE))

```


## The Weather Term
```{r}
weather_fit <- gamm(aChl_Median ~ 
                    wlPrecip +
                    te(lPrecip, lD_Median),
               data = plotdat, 
               na.action=na.omit, 
               method = 'REML',
               correlation=corAR1())
```

```{r}
q = predict(weather_fit$gam)
r = resid(weather_fit$gam)

plotdat <- plotdat %>%
  mutate(WeatherAdjust = q,
         cWeatherAdjust = scale(DOYAdjust, center=TRUE, scale=FALSE),
         Residual = r)
```

## Graphic Decomposition of the Time Series
```{r, fig.height = 9, fig.width = 5}

pivot_longer(plotdat, c(cObserved, cYrAdjust, cDOYAdjust, 
                        cWeatherAdjust, Residual), 
             names_to = 'kind', 
             values_to = 'value') %>%
  mutate(kind = factor(kind, 
                       levels = c('cObserved', 'cYrAdjust', 'cDOYAdjust', 
                        'cWeatherAdjust', 'Residual'),
                       labels = c('Observed', 'Year', 'Day of Year', 
                        'Weather', 'Residual'),)) %>%

ggplot( aes(sdate,value, color=kind)) + 
  geom_point() +
  # geom_smooth(aes(group = Yearf), se = FALSE, color = 'black', method = 'gam',
  #           formula = y~ s(x, bs = 'cs', k=3)) +
  facet_wrap(~kind, nrow=5, scales = 'fixed') +
  ylab('Chlorides (mg/l)') +
  xlab('Date') +
  ggtitle('Adjusted Geometric Means') +
  theme_cbep(base_size = 12)
```
